[
  {
    "id": "ml-001",
    "text": "What are the different types of Machine Learning?",
    "type": "conceptual",
    "difficulty": "easy",
    "follow_up_templates": [
      "How does supervised learning differ from unsupervised learning?",
      "Can you give an example of a reinforcement learning scenario?",
      "What makes semi-supervised learning different from the three main types?"
    ],
    "ideal_answer_summary": "There are three main types of machine learning: supervised learning uses labeled data where both input and output are known to train models that can make predictions on new data; unsupervised learning finds hidden patterns and structures in unlabeled data without knowing the correct output; and reinforcement learning trains agents to make decisions through trial and error, receiving rewards or penalties for their actions. Each type serves different purposes - supervised for prediction tasks, unsupervised for exploratory analysis and clustering, and reinforcement for sequential decision-making problems."
  },
  {
    "id": "ml-002",
    "text": "What is the bias-variance tradeoff in Machine Learning?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "How does model complexity affect bias and variance?",
      "What techniques can you use to address high variance in a model?",
      "Can you explain this concept using the example of polynomial regression?"
    ],
    "ideal_answer_summary": "The bias-variance tradeoff represents the fundamental tension between a model's ability to capture the true underlying relationship (bias) and its sensitivity to variations in the training data (variance). High bias occurs when a model is too simple and consistently misses the relevant patterns, leading to underfitting. High variance occurs when a model is too complex and learns random noise in the training data, causing it to perform poorly on new data (overfitting). The optimal model complexity minimizes the total error, which is the sum of bias squared, variance, and irreducible error. This tradeoff is central to model selection and regularization techniques."
  },
  {
    "id": "ml-003",
    "text": "What is overfitting, and how can it be prevented?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "What is underfitting and how is it different from overfitting?",
      "How would cross-validation help in preventing overfitting?",
      "What role does the training set size play in overfitting?"
    ],
    "ideal_answer_summary": "Overfitting occurs when a machine learning model learns the training data too well, including its noise and random fluctuations, resulting in poor generalization to new, unseen data. The model essentially memorizes the training examples rather than learning the underlying patterns. Prevention techniques include using more training data, applying regularization methods like L1 or L2 penalty terms, implementing cross-validation to better estimate model performance, reducing model complexity, using ensemble methods, and employing early stopping during training. The key is finding the right balance between model complexity and generalization ability."
  },
  {
    "id": "ml-004",
    "text": "What is regularization in machine learning and why is it useful?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "Can you explain the difference between L1 and L2 regularization?",
      "How does regularization relate to the bias-variance tradeoff?",
      "When would you choose Lasso over Ridge regression?"
    ],
    "ideal_answer_summary": "Regularization is a technique that adds a penalty term to the loss function to prevent overfitting by constraining model complexity. It discourages the model from learning overly complex patterns that might not generalize well. L1 regularization (Lasso) adds the sum of absolute values of parameters, promoting sparsity by driving some weights to zero, effectively performing feature selection. L2 regularization (Ridge) adds the sum of squared parameters, shrinking weights toward zero but not eliminating them entirely. Regularization increases bias slightly but significantly reduces variance, leading to better generalization performance on unseen data."
  },
  {
    "id": "ml-005",
    "text": "Explain the difference between precision and recall.",
    "type": "conceptual",
    "difficulty": "easy",
    "follow_up_templates": [
      "How do you calculate the F1-score from precision and recall?",
      "In what scenarios would you prioritize precision over recall?",
      "How does the precision-recall tradeoff affect model selection?"
    ],
    "ideal_answer_summary": "Precision measures the accuracy of positive predictions - it's the ratio of true positives to all predicted positives, answering 'Of all the cases I predicted as positive, how many were actually positive?' Recall measures the completeness of positive predictions - it's the ratio of true positives to all actual positives, answering 'Of all the actual positive cases, how many did I successfully identify?' High precision means fewer false positives, while high recall means fewer false negatives. There's typically a tradeoff between them - improving one often decreases the other, requiring careful consideration of the specific problem's cost structure."
  },
  {
    "id": "ml-006",
    "text": "What is a confusion matrix and what metrics can be derived from it?",
    "type": "conceptual",
    "difficulty": "easy",
    "follow_up_templates": [
      "How do you interpret a confusion matrix for a multi-class problem?",
      "What's the difference between sensitivity and specificity?",
      "How does class imbalance affect these metrics?"
    ],
    "ideal_answer_summary": "A confusion matrix is a table that visualizes the performance of a classification model by showing the counts of actual versus predicted classifications. From it, we can derive several key metrics: accuracy (correct predictions over total predictions), precision (true positives over predicted positives), recall or sensitivity (true positives over actual positives), specificity (true negatives over actual negatives), and F1-score (harmonic mean of precision and recall). The matrix helps identify which classes are being confused with others and provides insight into the types of errors the model is making, enabling targeted improvements."
  },
  {
    "id": "ml-007",
    "text": "What is cross-validation and why is it important?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "What's the difference between k-fold and stratified k-fold cross-validation?",
      "How does cross-validation help with hyperparameter tuning?",
      "What are the limitations of cross-validation?"
    ],
    "ideal_answer_summary": "Cross-validation is a statistical technique for assessing how well a machine learning model will generalize to independent data by partitioning the dataset into complementary subsets, training on some subsets and validating on others. It's crucial because it provides a more robust estimate of model performance than a single train-test split, helps detect overfitting, enables better model selection and hyperparameter tuning, and maximizes the use of available data for both training and validation. Common approaches include k-fold cross-validation and leave-one-out cross-validation, each with different computational trade-offs."
  },
  {
    "id": "ml-008",
    "text": "What is a hyperparameter and how do you tune them?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "What's the difference between grid search and random search?",
      "How does Bayesian optimization work for hyperparameter tuning?",
      "Why shouldn't you use the test set for hyperparameter tuning?"
    ],
    "ideal_answer_summary": "Hyperparameters are configuration settings external to the model that control the learning process but aren't learned from the training data, such as learning rate, tree depth, or regularization strength. Tuning involves systematically searching for optimal values using techniques like grid search (exhaustive search over predefined values), random search (sampling from distributions), or Bayesian optimization (using probabilistic models to guide the search). The process typically involves cross-validation to evaluate different combinations while avoiding overfitting to the validation set. Proper hyperparameter tuning can significantly improve model performance and is essential for maximizing the potential of machine learning algorithms."
  },
  {
    "id": "ml-009",
    "text": "What is a Convolutional Neural Network and why is it effective for image processing?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "How do convolutional layers differ from fully connected layers?",
      "What role do pooling layers play in CNNs?",
      "Why are CNNs translation invariant?"
    ],
    "ideal_answer_summary": "A Convolutional Neural Network is a specialized deep learning architecture designed for processing grid-like data such as images. CNNs use convolutional layers that apply learnable filters across the input to detect local features like edges, textures, and patterns. They're effective for images because they exploit spatial locality (nearby pixels are related), share parameters across the input (translation invariance), and use hierarchical feature learning (combining simple features into complex ones). Pooling layers reduce spatial dimensions while preserving important information, and the overall architecture mimics the visual cortex's hierarchical processing, making CNNs highly effective for computer vision tasks."
  },
  {
    "id": "ml-010",
    "text": "How do you handle imbalanced datasets?",
    "type": "design",
    "difficulty": "medium",
    "follow_up_templates": [
      "What's the difference between oversampling and undersampling?",
      "How does SMOTE work for generating synthetic samples?",
      "What evaluation metrics are most appropriate for imbalanced datasets?"
    ],
    "ideal_answer_summary": "Handling imbalanced datasets requires a multi-faceted approach including data-level techniques like oversampling the minority class, undersampling the majority class, or generating synthetic samples using methods like SMOTE. Algorithm-level approaches include using class weights to penalize misclassification of minority classes, ensemble methods like balanced random forests, and cost-sensitive learning. Evaluation should focus on metrics like precision, recall, F1-score, and AUC-ROC rather than accuracy. The choice of technique depends on the dataset size, the degree of imbalance, and the cost of different types of errors in the specific domain."
  },
  {
    "id": "ml-011",
    "text": "What is the curse of dimensionality?",
    "type": "conceptual",
    "difficulty": "hard",
    "follow_up_templates": [
      "How does high dimensionality affect distance-based algorithms?",
      "What techniques can help mitigate the curse of dimensionality?",
      "Why do nearest neighbor algorithms perform poorly in high dimensions?"
    ],
    "ideal_answer_summary": "The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces. As dimensionality increases, the volume of the space grows exponentially, making data increasingly sparse and distances between points more uniform. This affects machine learning algorithms in several ways: distance-based methods become less effective as all points appear equidistant, the amount of data needed to maintain statistical significance grows exponentially, and visualization becomes impossible. Mitigation techniques include dimensionality reduction methods like PCA, feature selection, regularization, and domain-specific feature engineering to focus on the most informative dimensions."
  },
  {
    "id": "ml-012",
    "text": "What's the difference between bagging and boosting ensemble methods?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "How does Random Forest implement bagging?",
      "What makes AdaBoost different from other boosting algorithms?",
      "When would you choose bagging over boosting?"
    ],
    "ideal_answer_summary": "Bagging (Bootstrap Aggregating) and boosting are two fundamental ensemble approaches that combine multiple models differently. Bagging trains multiple models independently on random subsets of the training data and averages their predictions, primarily reducing variance and preventing overfitting. Random Forest is a classic example. Boosting trains models sequentially, where each new model focuses on correcting the errors of previous models, primarily reducing bias by turning weak learners into strong ones. AdaBoost and Gradient Boosting are examples. Bagging is parallelizable and robust to outliers, while boosting can achieve higher accuracy but is more prone to overfitting and sensitive to noise."
  },
  {
    "id": "ml-013",
    "text": "What is Principal Component Analysis and how does it work?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "How do you decide how many principal components to keep?",
      "What's the difference between PCA and Linear Discriminant Analysis?",
      "What are the assumptions of PCA?"
    ],
    "ideal_answer_summary": "Principal Component Analysis is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving as much variance as possible. It works by finding the directions (principal components) in which the data varies the most, computed as eigenvectors of the covariance matrix. The first principal component captures the maximum variance, the second captures the maximum remaining variance orthogonal to the first, and so on. PCA is useful for visualization, noise reduction, and addressing the curse of dimensionality. However, it assumes linear relationships and the transformed features lose interpretability since they're combinations of original features."
  },
  {
    "id": "ml-014",
    "text": "Why is ReLU activation function commonly used in neural networks?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "What problems do sigmoid and tanh activations have that ReLU solves?",
      "What is the dying ReLU problem and how can it be addressed?",
      "How do Leaky ReLU and ELU compare to standard ReLU?"
    ],
    "ideal_answer_summary": "ReLU (Rectified Linear Unit) has become the default activation function because it effectively addresses the vanishing gradient problem that plagues sigmoid and tanh functions. ReLU's simple computation (max(0, x)) makes training faster, and its linear behavior for positive inputs allows gradients to flow backward without degradation. Unlike sigmoid functions that saturate at extreme values, ReLU maintains strong gradients for positive inputs, enabling deeper networks to train effectively. However, ReLU can suffer from the 'dying ReLU' problem where neurons output zero for all inputs, which variants like Leaky ReLU and ELU attempt to address while maintaining most of ReLU's benefits."
  },
  {
    "id": "ml-015",
    "text": "How do Transformers differ from RNNs in processing sequential data?",
    "type": "conceptual",
    "difficulty": "hard",
    "follow_up_templates": [
      "What is the self-attention mechanism in Transformers?",
      "How do Transformers handle long-range dependencies better than RNNs?",
      "Why are Transformers more parallelizable than RNNs?"
    ],
    "ideal_answer_summary": "Transformers revolutionized sequence processing by replacing recurrent connections with self-attention mechanisms, allowing parallel processing of entire sequences rather than the sequential processing required by RNNs. While RNNs process tokens one at a time and can struggle with long-range dependencies due to vanishing gradients, Transformers use self-attention to directly connect any two positions in a sequence, effectively modeling long-range dependencies. This parallel processing makes Transformers much more efficient to train on large datasets and enables them to capture complex relationships between all elements simultaneously. The attention mechanism also provides interpretability by showing which parts of the input the model focuses on."
  },
  {
    "id": "ml-016",
    "text": "What's the difference between discriminative and generative models?",
    "type": "conceptual",
    "difficulty": "medium",
    "follow_up_templates": [
      "Can you give examples of discriminative and generative models?",
      "When would you choose a generative model over a discriminative one?",
      "How do GANs fit into the generative model category?"
    ],
    "ideal_answer_summary": "Discriminative models learn the decision boundary between classes by modeling the conditional probability P(y|x), focusing on distinguishing between different categories. Examples include logistic regression, SVMs, and most neural networks used for classification. Generative models learn the joint probability distribution P(x,y) and can generate new data samples by modeling how the data is produced. Examples include Naive Bayes, GANs, and VAEs. Discriminative models typically achieve better classification accuracy since they focus solely on the decision boundary, while generative models provide more insight into data structure and can synthesize new samples, making them valuable for data augmentation and understanding underlying data distributions."
  },
  {
    "id": "ml-017",
    "text": "Describe your approach to implementing a sigmoid activation function.",
    "type": "coding",
    "difficulty": "easy",
    "follow_up_templates": [
      "How would you handle numerical stability issues with large inputs?",
      "What would be the derivative of sigmoid for backpropagation?",
      "How does sigmoid output relate to probability in logistic regression?"
    ],
    "ideal_answer_summary": "I would implement the sigmoid function using the mathematical formula one divided by one plus e to the negative x. For numerical stability with large negative inputs, I'd check if the input is very negative and return a value close to zero instead of computing the exponential directly, which could cause overflow. For large positive inputs, I'd reformulate to avoid computing large exponentials. The function should handle both single values and arrays of values, and I'd ensure the output is always between zero and one. The implementation should be efficient since sigmoid is used extensively in neural networks during both forward and backward passes."
  },
  {
    "id": "ml-018",
    "text": "How would you implement a softmax function?",
    "type": "coding",
    "difficulty": "medium",
    "follow_up_templates": [
      "How do you ensure numerical stability in softmax computation?",
      "What's the relationship between softmax and the argmax function?",
      "How does temperature scaling affect softmax outputs?"
    ],
    "ideal_answer_summary": "I would implement softmax by first subtracting the maximum value from all inputs to prevent numerical overflow, then computing the exponential of each element, and finally dividing each exponential by the sum of all exponentials. This normalization ensures the outputs sum to one and represent a valid probability distribution. The key insight is handling numerical stability by subtracting the max value, which doesn't change the final probabilities but prevents overflow when dealing with large input values. The function should work with both vectors and matrices, handling batch processing efficiently for neural network applications."
  },
  {
    "id": "ml-019",
    "text": "Explain your approach to calculating precision, recall, and F1-score from prediction lists.",
    "type": "coding",
    "difficulty": "medium",
    "follow_up_templates": [
      "How would you handle edge cases like no positive predictions?",
      "What modifications would you make for multi-class problems?",
      "How does micro-averaging differ from macro-averaging for these metrics?"
    ],
    "ideal_answer_summary": "I would iterate through both the actual and predicted lists simultaneously to count true positives, false positives, and false negatives. True positives are cases where both actual and predicted are positive, false positives are where predicted is positive but actual is negative, and false negatives are where actual is positive but predicted is negative. Precision equals true positives divided by the sum of true positives and false positives, recall equals true positives divided by the sum of true positives and false negatives, and F1-score is the harmonic mean of precision and recall. I'd handle edge cases like division by zero when there are no positive predictions or actual positives by returning appropriate default values."
  },
  {
    "id": "ml-020",
    "text": "Design a recommendation system for an e-commerce platform.",
    "type": "design",
    "difficulty": "hard",
    "follow_up_templates": [
      "How would you handle the cold start problem for new users and items?",
      "What metrics would you use to evaluate recommendation quality?",
      "How would you update recommendations in real-time as user behavior changes?"
    ],
    "ideal_answer_summary": "I would design a hybrid recommendation system combining collaborative filtering, content-based filtering, and contextual information. The system would include data collection components for user interactions, item metadata, and contextual features; a feature engineering pipeline to create user and item embeddings; multiple recommendation algorithms including matrix factorization for collaborative filtering and deep learning models for content-based recommendations; a real-time serving layer with caching for fast recommendations; and an evaluation framework using metrics like click-through rate, conversion rate, and diversity measures. The architecture would handle cold start problems through content-based recommendations and demographic clustering, while continuously learning from user feedback through online learning mechanisms."
  }
]
